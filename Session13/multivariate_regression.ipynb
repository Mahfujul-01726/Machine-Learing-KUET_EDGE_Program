{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xhRFqfn4lhs",
        "outputId": "1a9a41f5-03e4-485a-ed3f-7cda9988ffe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 299170939.6322265\n",
            "R-squared: 0.5213264965884375\n",
            "Mean Absolute Error: 16778.874056882545\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  price   R-squared:                       0.940\n",
            "Model:                            OLS   Adj. R-squared:                  0.894\n",
            "Method:                 Least Squares   F-statistic:                     20.78\n",
            "Date:                Thu, 02 Jan 2025   Prob (F-statistic):            0.00668\n",
            "Time:                        04:32:50   Log-Likelihood:                -88.270\n",
            "No. Observations:                   8   AIC:                             184.5\n",
            "Df Residuals:                       4   BIC:                             184.9\n",
            "Df Model:                           3                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const         9.2e+04   8.23e+04      1.118      0.326   -1.36e+05     3.2e+05\n",
            "bedrooms    6.983e+04    5.3e+04      1.317      0.258   -7.73e+04    2.17e+05\n",
            "bathrooms  -2.218e+04    3.4e+04     -0.652      0.550   -1.17e+05    7.22e+04\n",
            "sqft          52.6794    102.282      0.515      0.634    -231.302     336.661\n",
            "age        -4782.3564   3954.790     -1.209      0.293   -1.58e+04    6197.901\n",
            "==============================================================================\n",
            "Omnibus:                        1.714   Durbin-Watson:                   1.418\n",
            "Prob(Omnibus):                  0.424   Jarque-Bera (JB):                0.992\n",
            "Skew:                           0.594   Prob(JB):                        0.609\n",
            "Kurtosis:                       1.749   Cond. No.                     1.44e+17\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 1.34e-27. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=8\n",
            "  res = hypotest_fun_out(*samples, **kwds)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Creating a sample dataset\n",
        "data = {\n",
        "    'bedrooms': [3, 4, 2, 3, 4, 3, 2, 4, 3, 2],\n",
        "    'bathrooms': [2, 3, 1, 2, 3, 2, 1, 3, 2, 1],\n",
        "    'sqft': [2000, 2500, 1500, 1800, 2100, 1900, 1600, 2200, 2000, 1700],\n",
        "    'age': [10, 5, 8, 12, 6, 9, 11, 4, 7, 13],\n",
        "    'price': [300000, 400000, 250000, 280000, 410000, 320000, 230000, 390000, 350000, 260000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Performing multivariate linear regression\n",
        "X = df[['bedrooms', 'bathrooms', 'sqft', 'age']]\n",
        "y = df['price']\n",
        "\n",
        "X = sm.add_constant(X)  # adding a constant term to the predictor\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = sm.OLS(y_train, X_train).fit()\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate performance evaluation metrics\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MultipleLinearRegression:\n",
        "    def __init__(self):\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        X = np.c_[np.ones(n_samples), X]  # Add a column of ones for the bias term\n",
        "\n",
        "        # Calculate the coefficients using the OLS method\n",
        "        self.weights = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        X = np.c_[np.ones(n_samples), X]  # Add a column of ones for the bias term\n",
        "        return X.dot(self.weights)\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([[3, 2, 2000, 10],\n",
        "              [4, 3, 2500, 5],\n",
        "              [2, 1, 1500, 8],\n",
        "              [3, 2, 1800, 12],\n",
        "              [4, 3, 2100, 6],\n",
        "              [3, 2, 1900, 9],\n",
        "              [2, 1, 1600, 11],\n",
        "              [4, 3, 2200, 4],\n",
        "              [3, 2, 2000, 7],\n",
        "              [2, 1, 1700, 13]])\n",
        "\n",
        "y = np.array([300000, 400000, 250000, 280000, 410000, 320000, 230000, 390000, 350000, 260000])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training the model\n",
        "model = MultipleLinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "mse = np.mean((y_test - predictions) ** 2)\n",
        "r2 = 1 - (np.sum((y_test - predictions) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))\n",
        "mae = np.mean(np.abs(y_test - predictions))\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmgRLrw6QCqF",
        "outputId": "01c849a5-059a-403c-d6ae-f80746d795f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 34913145236.64707\n",
            "R-squared: -54.861032378635315\n",
            "Mean Absolute Error: 174002.22479623315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here's an updated version of your code that includes regularization using Ridge regression to prevent overfitting and improve the model's generalization\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class RidgeRegression:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        model = Ridge(alpha=self.alpha)\n",
        "        model.fit(X, y)\n",
        "        self.weights = model.coef_\n",
        "        self.bias = model.intercept_\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X.dot(self.weights) + self.bias\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([[3, 2, 2000, 10],\n",
        "              [4, 3, 2500, 5],\n",
        "              [2, 1, 1500, 8],\n",
        "              [3, 2, 1800, 12],\n",
        "              [4, 3, 2100, 6],\n",
        "              [3, 2, 1900, 9],\n",
        "              [2, 1, 1600, 11],\n",
        "              [4, 3, 2200, 4],\n",
        "              [3, 2, 2000, 7],\n",
        "              [2, 1, 1700, 13]])\n",
        "\n",
        "y = np.array([300000, 400000, 250000, 280000, 410000, 320000, 230000, 390000, 350000, 260000])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=46)\n",
        "\n",
        "# Training the model with Ridge regularization\n",
        "model = RidgeRegression(alpha=1.0)  # You can adjust the alpha value for regularization\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTkCjGXoThLT",
        "outputId": "516b643e-c025-4a56-d91a-5d3702a15976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 269464682.7406521\n",
            "R-squared: 0.8669310208688138\n",
            "Mean Absolute Error: 16268.75959990255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "class RidgeRegression:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        model = Ridge(alpha=self.alpha)\n",
        "        model.fit(X, y)\n",
        "        self.weights = model.coef_\n",
        "        self.bias = model.intercept_\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X.dot(self.weights) + self.bias\n",
        "\n",
        "    def select_alpha(self, X, y, alphas):\n",
        "        param_grid = {'alpha': alphas}\n",
        "        grid_search = GridSearchCV(Ridge(), param_grid, cv=5)\n",
        "        grid_search.fit(X, y)\n",
        "        self.alpha = grid_search.best_params_['alpha']\n",
        "        return self.alpha\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([[3, 2, 2000, 10], [4, 3, 2500, 5], [2, 1, 1500, 8], [3, 2, 1800, 12],\n",
        "              [4, 3, 2100, 6], [3, 2, 1900, 9], [2, 1, 1600, 11], [4, 3, 2200, 4],\n",
        "              [3, 2, 2000, 7], [2, 1, 1700, 13]])\n",
        "\n",
        "y = np.array([300000, 400000, 250000, 280000, 410000, 320000, 230000, 390000, 350000, 260000])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=46)\n",
        "\n",
        "# Initialize the RidgeRegression model\n",
        "model = RidgeRegression()\n",
        "\n",
        "# Grid search for alpha selection\n",
        "alphas = [0.1, 1.0, 10.0]  # Define a list of alpha values to search over\n",
        "best_alpha = model.select_alpha(X_train, y_train, alphas)\n",
        "\n",
        "print(\"Best Alpha:\", best_alpha)\n",
        "\n",
        "# Fit the model with the selected alpha\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1y72Rs2eFED",
        "outputId": "9e272b0b-638c-4849-de2e-4d2f9743ccc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Alpha: 0.1\n",
            "Mean Squared Error: 321540655.56954205\n",
            "R-squared: 0.8412144910767694\n",
            "Mean Absolute Error: 17915.37831219926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1107: UserWarning: One or more of the test scores are non-finite: [nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To implement L1 regularization (Lasso) in Python using the provided data, you can use the Lasso regression model from scikit-learn.\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "class LassoRegression:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        model = Lasso(alpha=self.alpha)\n",
        "        model.fit(X, y)\n",
        "        self.weights = model.coef_\n",
        "        self.bias = model.intercept_\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X.dot(self.weights) + self.bias\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([[3, 2, 2000, 10],\n",
        "              [4, 3, 2500, 5],\n",
        "              [2, 1, 1500, 8],\n",
        "              [3, 2, 1800, 12],\n",
        "              [4, 3, 2100, 6],\n",
        "              [3, 2, 1900, 9],\n",
        "              [2, 1, 1600, 11],\n",
        "              [4, 3, 2200, 4],\n",
        "              [3, 2, 2000, 7],\n",
        "              [2, 1, 1700, 13]])\n",
        "\n",
        "y = np.array([300000, 400000, 250000, 280000, 410000, 320000, 230000, 390000, 350000, 260000])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Training the model with Lasso regularization\n",
        "model = LassoRegression(alpha=1.0)  # You can adjust the alpha value for regularization\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtjRmThqkkit",
        "outputId": "2ef5f6f3-0f8e-4f73-fa8c-0dc12e0e2588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 299227851.8496748\n",
            "R-squared: 0.5212354370405203\n",
            "Mean Absolute Error: 16783.644586673705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "class RidgeRegression:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        model = Ridge(alpha=self.alpha)\n",
        "        model.fit(X, y)\n",
        "        self.weights = model.coef_\n",
        "        self.bias = model.intercept_\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X.dot(self.weights) + self.bias\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([[3, 2, 2000, 10],\n",
        "              [4, 3, 2500, 5],\n",
        "              [2, 1, 1500, 8],\n",
        "              [3, 2, 1800, 12],\n",
        "              [4, 3, 2100, 6],\n",
        "              [3, 2, 1900, 9],\n",
        "              [2, 1, 1600, 11],\n",
        "              [4, 3, 2200, 4],\n",
        "              [3, 2, 2000, 7],\n",
        "              [2, 1, 1700, 13]])\n",
        "\n",
        "y = np.array([300000, 400000, 250000, 280000, 410000, 320000, 230000, 390000, 350000, 260000])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=46)\n",
        "\n",
        "# Training the model with Ridge regularization\n",
        "model = RidgeRegression(alpha=1.0)  # You can adjust the alpha value for regularization\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62ihMQO1lBFB",
        "outputId": "4353c9d7-7f14-4a55-de73-ff43fa148b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 269464682.7406521\n",
            "R-squared: 0.8669310208688138\n",
            "Mean Absolute Error: 16268.75959990255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we need to select the good features. So used backward elimination is needed to select good feature\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Creating a sample dataset\n",
        "data = {\n",
        "    'X1': [3, 4, 2, 3, 4, 3, 2, 4, 3, 2],\n",
        "    'X2': [2, 3, 1, 2, 3, 2, 1, 3, 2, 1],\n",
        "    'X3': [2000, 2500, 1500, 1800, 2100, 1900, 1600, 2200, 2000, 1700],\n",
        "    'X4': [10, 5, 8, 12, 6, 9, 11, 4, 7, 13],\n",
        "    'y': [300000, 400000, 250000, 280000, 410000, 320000, 230000, 390000, 350000, 260000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Performing multiple linear regression with backward elimination\n",
        "X = df[['X1', 'X2', 'X3', 'X4']]\n",
        "y = df['y']\n",
        "\n",
        "# Adding a constant term to the predictor\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Backward Elimination\n",
        "cols = list(X.columns)\n",
        "while len(cols) > 0:\n",
        "    X_opt = X[cols]\n",
        "    model = sm.OLS(y, X_opt).fit()\n",
        "    p_values = model.pvalues\n",
        "    max_p_value = p_values.drop('const').max()\n",
        "    if max_p_value > 0.05:\n",
        "        cols.remove(p_values.idxmax())\n",
        "    else:\n",
        "        break\n",
        "\n",
        "selected_features = cols\n",
        "print(\"Selected Features after Backward Elimination:\", selected_features)\n",
        "\n",
        "# Fit the final model\n",
        "final_model = sm.OLS(y, X[selected_features]).fit()\n",
        "\n",
        "# Make predictions\n",
        "predictions = final_model.predict(X[selected_features])\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y, predictions)\n",
        "r2 = r2_score(y, predictions)\n",
        "mae = mean_absolute_error(y, predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "\n",
        "print(final_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qkxu9QvFzr3",
        "outputId": "c2943972-f80b-45db-ed10-995d1e8ee41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features after Backward Elimination: ['const', 'X1']\n",
            "Mean Squared Error: 362333333.3333334\n",
            "R-squared: 0.9068312333933316\n",
            "Mean Absolute Error: 15200.000000000004\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.907\n",
            "Model:                            OLS   Adj. R-squared:                  0.895\n",
            "Method:                 Least Squares   F-statistic:                     77.87\n",
            "Date:                Wed, 01 Jan 2025   Prob (F-statistic):           2.14e-05\n",
            "Time:                        18:19:24   Log-Likelihood:                -112.73\n",
            "No. Observations:                  10   AIC:                             229.5\n",
            "Df Residuals:                       8   BIC:                             230.1\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const         8.9e+04   2.69e+04      3.306      0.011    2.69e+04    1.51e+05\n",
            "X1          7.667e+04   8688.274      8.824      0.000    5.66e+04    9.67e+04\n",
            "==============================================================================\n",
            "Omnibus:                        0.841   Durbin-Watson:                   2.070\n",
            "Prob(Omnibus):                  0.657   Jarque-Bera (JB):                0.343\n",
            "Skew:                          -0.427   Prob(JB):                        0.842\n",
            "Kurtosis:                       2.693   Cond. No.                         13.6\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n",
            "  res = hypotest_fun_out(*samples, **kwds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([[3, 2, 2000, 10],\n",
        "              [4, 3, 2500, 5],\n",
        "              [2, 1, 1500, 8],\n",
        "              [3, 2, 1800, 12],\n",
        "              [4, 3, 2100, 6],\n",
        "              [3, 2, 1900, 9],\n",
        "              [2, 1, 1600, 11],\n",
        "              [4, 3, 2200, 4],\n",
        "              [3, 2, 2000, 7],\n",
        "              [2, 1, 1700, 13]])\n",
        "\n",
        "y = np.array([300000, 400000, 250000, 280000, 410000, 320000, 230000, 390000, 350000, 260000])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=46)\n",
        "\n",
        "# Fit the Lasso regression model with L1 regularization\n",
        "alpha = 0.1  # Regularization parameter (use 0.1 then show the result)\n",
        "lasso = Lasso(alpha=alpha)\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = lasso.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Lasso Coefficients:\", lasso.coef_)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-oNRSJwUka6",
        "outputId": "aa182047-6efa-4086-e484-23519cc3b9cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 333035476.44719684\n",
            "Lasso Coefficients: [ 5.02783783e+04  0.00000000e+00  4.05430630e+01 -5.68871832e+03]\n",
            "R-squared: 0.8355380363223719\n",
            "Mean Absolute Error: 18205.73418637592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#grid search with L1 regularization\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "class LassoRegression:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        model = Lasso(alpha=self.alpha)\n",
        "        model.fit(X, y)\n",
        "        self.weights = model.coef_\n",
        "        self.bias = model.intercept_\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X.dot(self.weights) + self.bias\n",
        "\n",
        "    def select_alpha(self, X, y, alphas):\n",
        "        param_grid = {'alpha': alphas}\n",
        "        grid_search = GridSearchCV(Lasso(), param_grid, cv=5)\n",
        "        grid_search.fit(X, y)\n",
        "        self.alpha = grid_search.best_params_['alpha']\n",
        "        return self.alpha\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([[3, 2, 2000, 10], [4, 3, 2500, 5], [2, 1, 1500, 8], [3, 2, 1800, 12],\n",
        "              [4, 3, 2100, 6], [3, 2, 1900, 9], [2, 1, 1600, 11], [4, 3, 2200, 4],\n",
        "              [3, 2, 2000, 7], [2, 1, 1700, 13]])\n",
        "\n",
        "y = np.array([300000, 400000, 250000, 280000, 410000, 320000, 230000, 390000, 350000, 260000])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=46)\n",
        "\n",
        "# Initialize the LassoRegression model\n",
        "model = LassoRegression()\n",
        "\n",
        "# Grid search for alpha selection\n",
        "alphas = [0.1, 1.0, 10.0]  # Define a list of alpha values to search over\n",
        "best_alpha = model.select_alpha(X_train, y_train, alphas)\n",
        "\n",
        "print(\"Best Alpha:\", best_alpha)\n",
        "\n",
        "# Fit the model with the selected alpha\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDB8oY6gho7R",
        "outputId": "82e1ad60-9b0b-4a0a-d466-6abaf96bcddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Alpha: 0.1\n",
            "Mean Squared Error: 333035476.44719684\n",
            "R-squared: 0.8355380363223719\n",
            "Mean Absolute Error: 18205.73418637592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1107: UserWarning: One or more of the test scores are non-finite: [nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}